{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1088,
      "metadata": {
        "id": "GYwLWhyMPBXA"
      },
      "outputs": [],
      "source": [
        "#Import Libraries\n",
        "import math\n",
        "import nltk\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import os\n",
        "import re\n",
        "import operator\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "#nltk.download ('all')\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "wordlemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1089,
      "metadata": {},
      "outputs": [],
      "source": [
        "#input article 1 (newspaper article)\n",
        "#f = open (\"Flood_Malaysia.txt\", \"r\")\n",
        "#text = (f.read())\n",
        "#print (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1090,
      "metadata": {},
      "outputs": [],
      "source": [
        "#input article 2 (research paper)\n",
        "#f = open (\"Covid_article.txt\", \"r\")\n",
        "#text = (f.read())\n",
        "#print (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1091,
      "metadata": {},
      "outputs": [],
      "source": [
        "#input article 3 (web article)\n",
        "f = open (\"Economy_Malaysia.txt\", \"r\") #\n",
        "text = (f.read())\n",
        "#print (text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1092,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Text preprocessing\n",
        "tokenized_sentence = sent_tokenize(text)\n",
        "text = re.sub(r'\\d+', '', text)\n",
        "tokenized_words_with_stopwords = word_tokenize(text)\n",
        "tokenized_words = [word for word in tokenized_words_with_stopwords if word not in Stopwords]\n",
        "tokenized_words = [word for word in tokenized_words if len(word) > 1]\n",
        "tokenized_words = [word.lower() for word in tokenized_words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1093,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Removal of special character \"^a-zA-Z0-9\\s\"\n",
        "def remove_special_characters(text):\n",
        "    regex = r'[^a-zA-Z0-9\\s]'\n",
        "    text = re.sub(regex,'',text)\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1094,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c8336HMuvSl3",
        "outputId": "e9c62127-4873-410c-e850-023932c7cde2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentences: \n"
          ]
        }
      ],
      "source": [
        "# Sentence Tokenize\n",
        "'''\n",
        "    We already have a sentence tokenizer, so we just need \n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "'''\n",
        "sentences = sent_tokenize(text)\n",
        "total_documents = len(sentences)\n",
        "print(\"Sentences: \")\n",
        "#print(sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1095,
      "metadata": {
        "id": "8pgzObs2VSlk"
      },
      "outputs": [],
      "source": [
        "# Function to pos tag all the words in the text and returns only the nouns and verbs from the text\n",
        "def pos_tagging(text):\n",
        "    pos_tag = nltk.pos_tag(text.split())\n",
        "    pos_tagged_noun_verb = []\n",
        "    for word,tag in pos_tag:\n",
        "        if tag == \"NN\" or tag == \"NNP\" or tag == \"NNS\" or tag == \"VB\" or tag == \"VBD\" or tag == \"VBG\" or tag == \"VBN\" or tag == \"VBP\" or tag == \"VBZ\":\n",
        "            pos_tagged_noun_verb.append(word)\n",
        "    return pos_tagged_noun_verb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1096,
      "metadata": {
        "id": "PlLkQyiSPfIy"
      },
      "outputs": [],
      "source": [
        "#Create frequency table\n",
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we only used the words that are not part of the stopWords array.\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1097,
      "metadata": {
        "id": "pMcQE2DEPja8"
      },
      "outputs": [],
      "source": [
        "#Create frequency matrix of the words in each sentence\n",
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1098,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f5matg8vGc9",
        "outputId": "8e3f6045-1995-4465-accd-42c6c2039847"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency matrix: \n"
          ]
        }
      ],
      "source": [
        "freq_matrix = _create_frequency_matrix(sentences)\n",
        "print(\"Frequency matrix: \")\n",
        "#print(freq_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1099,
      "metadata": {
        "id": "LbC_yeSVPmwp"
      },
      "outputs": [],
      "source": [
        "#Create term- frequency matrix by calculating tf for each word\n",
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1100,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUuO1oA3vmhT",
        "outputId": "d687823c-9238-4f84-e991-fc8476097e7e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf matrix: \n"
          ]
        }
      ],
      "source": [
        "#Calculate TermFrequency and generate a matrix\n",
        "#Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document\n",
        "tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "print(\"tf matrix: \")\n",
        "#print(tf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1101,
      "metadata": {
        "id": "3jNKK5GcPpZC"
      },
      "outputs": [],
      "source": [
        "#Function for per word document to score calculation by idf matrix\n",
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1102,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39TFaVwvvzJF",
        "outputId": "e46ab64e-c29f-4fff-9d98-cb59aa891d57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Count doc per word matrix: \n"
          ]
        }
      ],
      "source": [
        "#creating table for per words apeears in documents\n",
        "count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "print(\"Count doc per word matrix: \")\n",
        "#print(count_doc_per_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1103,
      "metadata": {
        "id": "vtVc5_6JPuAd"
      },
      "outputs": [],
      "source": [
        "#Inverse document frequency matrix calculation each word in the sentence\n",
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzgjO65SwBGW",
        "outputId": "23d2ea25-eca8-4706-fb00-676db83545ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "idf matrix: \n"
          ]
        }
      ],
      "source": [
        "#Calculate IDF and generate a matrix\n",
        "#Inverse document frequency (IDF) is how unique or rare a word is.\n",
        "    \n",
        "idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "print(\"idf matrix: \")\n",
        "#print(idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1105,
      "metadata": {
        "id": "6tbLzeMRPxRU"
      },
      "outputs": [],
      "source": [
        "# A function for term frequency-inverse document frequency matrix by calculating tf-idf\n",
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf-idf matrix: \n"
          ]
        }
      ],
      "source": [
        "# Calculate TF-IDF and generate a matrix\n",
        "tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "print(\"tf-idf matrix: \")\n",
        "#print(tf_idf_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1107,
      "metadata": {
        "id": "BRIEltQHPykb"
      },
      "outputs": [],
      "source": [
        "#Scoring the sentecne with distinct algorithm, here we have used tf-idf score\n",
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word \n",
        "    in a sentence divided by total no of words in a sentence.\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-OaiaYGwV-V",
        "outputId": "03f7fe47-3ce7-4420-f0a4-c76f6f0b6c8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sentence scores: \n"
          ]
        }
      ],
      "source": [
        " #score the sentences\n",
        "sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "print(\"sentence scores: \")\n",
        "#print(sentence_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1109,
      "metadata": {
        "id": "OZsO4COJP16F"
      },
      "outputs": [],
      "source": [
        "#Find threshold by calculating the average sentence score.\n",
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GjX_avWwbjs",
        "outputId": "3c41d640-108a-4a66-cda2-8a442e1caae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold: \n",
            "0.06860135204014838\n"
          ]
        }
      ],
      "source": [
        "#Find the threshold\n",
        "threshold = _find_average_score(sentence_scores)\n",
        "print(\"Threshold: \")\n",
        "print(threshold)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1111,
      "metadata": {
        "id": "qNEXTqaMP6jP"
      },
      "outputs": [],
      "source": [
        "#Generate the final summary of the praghraph\n",
        "\n",
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1112,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eT73lVc6RN0B",
        "outputId": "b217b884-88e4-4365-eace-7835f5e95ff4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary:\n",
            " More significantly, at least on the part of those being relocated, it provokes social disruption and upheaval when people are bundled into an alien environment. Examines the effectiveness of government‚Äêrun permanent relocation schemes as a response to flood hazards in Malaysia. In Malaysia, the DID is the recognised flood management authority in the country. The DID, however, is a department dominated by engineers whose traditional training is to control floods. In reality, however, it coordinates operations at the national level and overlooks operations at the state level. Much of the operations in each state are left to be run by the respective state authorities. It is also more of a welfare body than it is a flood management organisation. The DRPC coordinates all relief operations from the Malaysian Control Centre in Kuala Lumpur.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def run_summarization(text):\n",
        "\n",
        "    #Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.1 * threshold)\n",
        "    return summary\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    result = run_summarization(text)\n",
        "    print(\"Summary:\")\n",
        "    print(result)\n",
        "    outF = open('SummaryEconomy_Malaysia.txt',\"w\")\n",
        "    outF.write(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "572SrtxYzCdj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "NLP Final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
